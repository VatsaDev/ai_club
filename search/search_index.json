{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"KU AI Club","text":"<p>KU AI Organization will be project-driven and members are encouraged to collaborate and share their knowledge of computer science, mathematics, engineering, finance, natural sciences, and any other relevant fields to develop projects of their choosing. </p> <p>In addition to providing a space to work on interesting AI/ ML projects with other students, this organization will provide opportunities to compete in machine learning competitions and find opportunities to learn more from an industry perspective through public webinars and speaker events. Depending on interest we will also host competitions with other clubs. </p> <p>Students of all expertise levels, majors, and years are welcome!</p>"},{"location":"leaders.html","title":"Club Executives","text":"Spencer Weishaar President Vatsa Pandey Vice-President John Rader Treasurer"},{"location":"projects.html","title":"Projects","text":""},{"location":"resources.html","title":"Competition and Development resources","text":"<p>Honorary mention to Huggingface, a platform where you can store an infinite amount of models and data</p>"},{"location":"resources.html#neural-network-development-resources","title":"Neural Network Development resources","text":"<p>Resources here will be model based, Ideally take a look at the tutorials before coming here</p>"},{"location":"resources.html#transformer-models","title":"Transformer models","text":""},{"location":"resources.html#diffusion-models","title":"Diffusion models","text":""},{"location":"resources.html#competitions","title":"Competitions","text":"<p>The most common location for every competition, Kaggle</p> <p>Here's a couple designed to get you learning the format and maybe explore new kinds of NNs and data science you haven't seen before:</p> <ul> <li>MNIST Data digit recognization, basically the very first useable NN anyone makes, but you can also explore the code and discussions to figure out how people optimize every aspect of a NN</li> <li>Titanic competition, basically the main intro to kaggle</li> <li>Housing prices, more around data science and non NN methods, but will definitely teach you that a NN is not suitable for every situation, and there are much better and lighter options out there</li> <li>Space titanic, Titanic 2, try it after you're done with the first one</li> </ul> <p>Still learner based competitions, but these are either meant for learning specific NNs or specific hardware</p> <ul> <li>LLM classification, a starting point for anyone wanting to classify complex text </li> <li>I\u2019m Something of a Painter Myself, If you're interested in AI generated imagery of any kind, this is your starting point</li> <li>Petals to the Metal, an intro to TPU chips, which google has in massive supply and provides large amounts to for student projects (look up google TRC), good to know how to use their hardware for when you need to scale up a project</li> </ul>"},{"location":"tutorials.html","title":"Tutorials","text":""},{"location":"tutorials.html#a-general-intro-to-machine-learning","title":"A general intro to machine learning","text":"<p>getting the basics down -&gt; Making your first neural network</p>"},{"location":"tutorials.html#looking-at-your-problem-as-a-shape","title":"Looking at your problem as a shape","text":"<p>quickly turning your problem into a model</p> <p>We've come across a variety of people who have a problem they would like to solve that they think AI can help with.</p> <p>A couple examples of this are:</p> <ul> <li>estimating the weight of a fish based off of a screenshot</li> <li>optimizing a power grid to predict what areas will need more power in the next few hours</li> <li>attempting to solve 2d puzzles or other arcade games</li> </ul> <p>Most people come in with the assumption that they will need a completely custom neural network and a lot of knowledge to make the smallest of demos, but in reality, most problems can fit within the scope of common model types.</p>"},{"location":"tutorials.html#start-with-representing-the-problem","title":"Start with representing the problem","text":"<p>Think of how you can represent your data, what shapes seem the most natural or obvious, then consult the table below</p> model type what the input looks like how you usually prepare the data mlp a simple list of numbers (like <code>[5, 7, 3]</code>) put everything into a list, scale values cnn a grid/array of values (like a table or an image) arrange data into rows/columns, add depth if needed rnn/lstm/gru a sequence of steps (like words in a sentence, or time series) make all sequences the same length, turn words/items into numbers transformer a sequence with positions (like a sentence where order matters) same as above, but also give the model info about position/order autoencoder anything, but input and output have the same shape clean/normalize data so it can be reconstructed gnn a set of points with connections (a network/graph) describe which points are linked and what each point's values are <p>Links to each NN (WIP):</p> <ul> <li>mlp tutorial (WIP)</li> <li>cnn tutorial (WIP)</li> <li>rnn tutorial (WIP)</li> <li>transformer tutorial (WIP)</li> <li>autoencoder tutorial (WIP)</li> <li>gnn tutorial (WIP)</li> </ul>"},{"location":"tutorials/nn_base.html","title":"Making your first Neural Network","text":"<p>This is a simplified intro, enough to get you to understand optimization and how the learning process is affected, for a deeper explanation, look into Neural Networks from Scratch in Python</p> <p></p> <p>This is the simplest form of Neural Network. </p> <p>Input \u2192 Processing \u2192 Output</p> <p>You might notice that this is really similar to a mathematical function X \u2192 Y with a formula in the middle. </p> <p>A Neural Network splits this into three sections:</p> <ul> <li>input layer: where you feed in numbers (your data).  </li> <li>hidden layers: where the math happens (processing).  </li> <li>output layer: where you get predictions/answers.</li> </ul>"},{"location":"tutorials/nn_base.html#step-1-input-layer","title":"step 1: input layer","text":"<p>let's say you want to predict whether a student passes or fails based on how many hours they studied and how much they slept.</p> <p>example input: - hours studied = <code>5</code> - hours slept = <code>7</code></p> <p>we put that into a list:</p> <pre><code>inputs = [5, 7]\n</code></pre>"},{"location":"tutorials/nn_base.html#step-2-weights-and-bias","title":"step 2: weights and bias","text":"<p>a neural network wont just take the input directly, it uses weights and biases to adjust them. Weights are a way to scale the number, a way to see how \"important\" each input is, and a bias is a single number that is used to shift the inputs</p> <p>Here we just use random starting values:</p> <pre><code>weights = [0.1, 0.2]\nbias = 0.5\n</code></pre>"},{"location":"tutorials/nn_base.html#step-3-the-dot-product","title":"step 3: the dot product","text":"<p>we need to get the dot product of the inputs and the weights, shifted by the bias. You've probably seen a 2x2 dot product in linear algebra or basic calc</p> <p></p> <p>In practice, it looks like this:</p> <pre><code>output = (inputs[0] * weights[0]) + (inputs[1] * weights[1]) + bias\nprint(output)\n</code></pre>"},{"location":"tutorials/nn_base.html#step-4-the-activation-function","title":"step 4: the activation function","text":"<p>if we stop here, the network is just doing linear math. to make it handle complex patterns, we apply a non-linear function called an activation function.</p> <p>There are many non-linear activation functions, but the one we are using here is ReLU, which looks like this:</p> <p>ReLU(x) = max(0, x)</p> <p></p> <p>In my personal experience the SiLU function works best, often a 5-10% performance jump on relu, it looks like this:</p> <p></p> <p>we can add the relu function to our code like this:</p> <pre><code>def relu(x):\n    return max(0, x)\n\nactivated_output = relu(output)\nprint(activated_output)\n</code></pre>"},{"location":"tutorials/nn_base.html#step-5-the-output-layer","title":"step 5: the output layer","text":"<p>since we have a pass/fail situation, we have two outputs, represented by 0 and 1, to force our outputs in the 0 to 1 range, we can use a sigmoid function, which looks like this:</p> <p>we can use it in our code through the math library:</p> <pre><code>import math\n\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nfinal_output = sigmoid(output)\nprint(final_output)\n</code></pre> <p><code>final_output</code> will be between 0 and 1, like a probability (ex. 0.8 \u2192 80% chance of passing)</p> <p>putting it all together for now with a couple variable name changes to represent the diagram, gives us the structure of the neural network</p> <pre><code>import math\n\n# inputs\ninputs = [5, 7]\n\n# weights and bias (randomly chosen for now)\nweights = [0.1, 0.2]\nbias = 0.5\n\n# step 1: linear combination\noutput = (inputs[0] * weights[0]) + (inputs[1] * weights[1]) + bias\n\n# step 2: activation\ndef relu(x):\n    return max(0, x)\n\nhidden = relu(output)\n\n# step 3: output activation (sigmoid)\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nfinal_output = sigmoid(hidden)\nprint(\"prediction:\", final_output)\n</code></pre>"},{"location":"tutorials/nn_base.html#step-6-actually-training-the-neural-net","title":"step 6: actually training the neural net","text":"<p>rn the weights/bias are random, so predictions are useless. training = adjusting them until outputs match the right answers.</p> <p>process:</p> <p>give the network data (inputs), compare output with the correct answer (error), adjust weights/bias a little to reduce error, repeat this thousands of times.</p> <p>this is called gradient descent + backpropagation.</p> <p>we wont be diving into the calculus here, as its a more advanced topic, but you can find a detailed breakdown here</p> <p>The loss plot usually looks like this as the NN tries to reach a loss of zero:</p> <p></p> <p>how does the NN know how to move in the right direction to reduce loss?</p> <p>when the NN makes a prediction, theres a certain amount of error between it and the true known answer, we get the mean squared error using code like this:</p> <p><code>MSE = (truth-pred)**2</code></p> <p>This forms a chart that looks like this:</p> <p></p> <p>the closer you are to the true answer, the lower on the graph you'll be.</p> <p>this learning process moves the neural networks weights in a direction, positive/negative, and that amount is scaled by the <code>learning_rate</code> or <code>lr</code>, which is a number, usually between <code>1e-3</code> and <code>1e-5</code>, that makes sure we dont update to much too fast</p> <p>When you move too slow, you'll end up taking too long, wasting expensive GPU time when working with larger models:</p> <p></p> <p>On the other hand, move too fast, and you worsen the model:</p> <p></p>"},{"location":"tutorials/nn_base.html#wrapping-up","title":"wrapping up","text":"<p>we are using pytorch, a standard NN library, for the final code, simply because the backpropagation involved in training is complicated and time consuming, and torch will handle that for us here</p> <p>the final code looks like this:</p> <pre><code>import torch\nimport torch.nn as nn\n\n# dataset (hours studied, hours slept) \u2192 pass/fail\nX = torch.tensor([[5.0, 7.0], [1.0, 2.0], [10.0, 6.0]])\ny = torch.tensor([[1.0], [0.0], [1.0]])\n\n# model\nmodel = nn.Sequential(\n    nn.Linear(2, 1),   # 2 inputs \u2192 1 output\n    nn.Sigmoid()       # squash into 0\u20131\n)\n\n# loss + optimizer\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1) # higher here because we are a simple, tiny neural network \n\n# training loop\nfor epoch in range(100):\n    y_pred = model(X)\n    loss = loss_fn(y_pred, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(\"prediction for [5,7]:\", model(torch.tensor([5.0,7.0])))\n</code></pre>"}]}